{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f466516",
   "metadata": {},
   "source": [
    "Implement Newton's method for minimizing the function:\n",
    "\n",
    "$f(x) = \\frac{1}{2} x^T Q x - \\sum_{i = 1}^n \\log(x_i)$\n",
    "\n",
    "subject to $Ax = b$.\n",
    "\n",
    "Here $x \\in R^n$ is the variable, $Q \\in S^n$ is a positive definite matrix and $A \\in R^{m \\times n}$. To generate the problem data $Q$, let $Q = B^T B$ for some random $B \\in R^{n \\times n}$. To generate $A$ and $b$, let $x_0 = \\boldsymbol{1}$, generate $A$ randomly and then set $b = Ax_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc01e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f271c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd4ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the objective function\n",
    "\n",
    "def f(x, Q):\n",
    "    return 0.5 * x.T @ Q @ x - np.sum(np.log(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128fe43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the gradient of the objective function\n",
    "\n",
    "def gradient_f(x, Q):\n",
    "    return Q @ x - 1/x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b9a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Hessian of the objective function\n",
    "\n",
    "def hessian_f(x, Q):\n",
    "    n = len(x)\n",
    "    H = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        H[i, i] = 1 / x[i]**2\n",
    "    return Q + H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a972dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Newton's method for optimization\n",
    "\n",
    "def newtons_method(x_init, Q, A, b, tol = 1e-10, max_iter = 100):\n",
    "    \n",
    "    x = x_init\n",
    "    alpha = 0.2\n",
    "    beta = 0.5\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        \n",
    "        grad = gradient_f(x, Q)\n",
    "        hess = hessian_f(x, Q)\n",
    "        \n",
    "        #calculate coefficient matrix of KKT equation system\n",
    "        m, n = A.shape\n",
    "        top_row = np.hstack((hess, A.T))\n",
    "        zeros_matrix = np.zeros((m, m)) \n",
    "        bottom_row = np.hstack((A, zeros_matrix))\n",
    "        coeff = np.vstack((top_row, bottom_row))\n",
    "        \n",
    "        #calculate right-hand-side of KKT equation system\n",
    "        zeros_vector = np.zeros((m, 1))\n",
    "        rhs = np.vstack((-grad, zeros_vector))\n",
    "        \n",
    "        #solve the equation system for the Newton step\n",
    "        sol = np.linalg.solve(coeff, rhs)\n",
    "        newton_step = sol[0:len(grad)] #the Newton step is the first element of sol\n",
    "        \n",
    "        newton_step_norm = np.linalg.norm(newton_step)\n",
    "        stop_criterion = (newton_step_norm ** 2)/2 #calculate stop criterion, a.k.a lambda squared divided by two\n",
    "        print(\"Decrement: \", stop_criterion)\n",
    "        print(\"Residual: \" ,np.linalg.norm(A@x - b))\n",
    "        \n",
    "        if stop_criterion <= tol: #exit newtons method if the stop criterion is smaller than our tolerance level \n",
    "            break\n",
    "        \n",
    "        #if we the norm of the newton step isn't sufficiently small we iterate with step length t\n",
    "        #making sure no component exit the dominion of x by adjusting step length t\n",
    "        t = 1.0 #initial step length\n",
    "        while True:\n",
    "            x_new = x + t*newton_step #calculate new x\n",
    "            if np.all(x_new > 0): #if all components are larger than zero we perform backtracking line search with alpha and beta\n",
    "                while f(x_new, Q) > f(x, Q) + alpha*t*grad.T @ newton_step: #perform back tracking line search for as long as the new x does not give us a better function value\n",
    "                    t *= beta \n",
    "                    x_new = x + t*newton_step \n",
    "                x = x_new \n",
    "                break #the outer while loop is broken as we have found an x that is in the dominion and minimizes the objective function\n",
    "            else: #if the new x has components that are not larger than zero we decrease t\n",
    "                t /= 2\n",
    "              \n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dab1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the conditions for your problem\n",
    "\n",
    "#size of problem\n",
    "n = 10\n",
    "m = 5\n",
    "#initial point\n",
    "x0 = np.ones((n, 1))\n",
    "\n",
    "#generate a symmetric positive definite matrix Q\n",
    "B = np.random.rand(n, n)\n",
    "Q = B.T @ B\n",
    "\n",
    "#generate a random matrix A\n",
    "A = np.random.rand(m, n)\n",
    "\n",
    "#decide b\n",
    "b = A @ x0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a1430d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrement:  16.250509646204137\n",
      "Residual:  0.0\n",
      "Decrement:  5.258368391198106\n",
      "Residual:  8.881784197001252e-16\n",
      "Decrement:  0.6243018275673728\n",
      "Residual:  1.3322676295501878e-15\n",
      "Decrement:  0.038614504096929535\n",
      "Residual:  2.5121479338940403e-15\n",
      "Decrement:  0.00010389184850943375\n",
      "Residual:  4.463041323674983e-15\n",
      "Decrement:  1.1109427290340047e-08\n",
      "Residual:  3.9968028886505635e-15\n",
      "Decrement:  1.126308301932195e-15\n",
      "Residual:  5.273267194922012e-15\n",
      "The vector x that minimizes the objective function is \n",
      " [[0.12993267]\n",
      " [1.26333393]\n",
      " [0.27451132]\n",
      " [3.20270049]\n",
      " [0.10538557]\n",
      " [0.84610506]\n",
      " [0.66394907]\n",
      " [0.95279649]\n",
      " [0.26852794]\n",
      " [1.56646077]]\n",
      "The function value of that vector x is equal to \n",
      " [[111.67517871]]\n"
     ]
    }
   ],
   "source": [
    "#Solve the problem using Newton's method\n",
    "\n",
    "x_optimal = newtons_method(x0, Q, A, b)\n",
    "\n",
    "print(\"The vector x that minimizes the objective function is \\n\", x_optimal)\n",
    "print(\"The function value of that vector x is equal to \\n\", f(x_optimal, Q))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
